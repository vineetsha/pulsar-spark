package com.flipkart.examples

/*
* Usage: NetworkWordCount <hostname> <port>
  * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
    *    run on local:
    *    nc -l -v 7777
    *    run on loacl:
    *    bin/spark-submit --total-executor-cores 10 --class "NetworkWordCount" --driver-class-path /Users/sharma.varun/flipkart/gdp/spark-1.1.0/spark-cassandra-connector/spark-cassandra-connector-java/target/scala-2.10/spark-cassandra-connector-java-assembly-1.2.0-SNAPSHOT.jar /Users/sharma.varun/flipkart/gdp/ekl-compass/spark_streaming/target/spark-jobs-1.0-SNAPSHOT.jar localhost 7777
    *    run on server:
    *    /opt/flipkart/spark/spark-1.1.0/bin/spark-submit --total-executor-cores 2 --class "NetworkWordCount" --master spark://10.75.123.103:7077 --driver-class-path /opt/flipkart/spark-cassandra-connector/spark-cassandra-connector-java/target/scala-2.10/spark-cassandra-connector-java-assembly-1.2.0-SNAPSHOT.jar /home/sharma.varun/simple-project_2.10-1.0.jar 172.20.201.175 7777
    */

import com.datastax.spark.connector.SomeColumns
import com.datastax.spark.connector.streaming._
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}

    object NetworkWordCount {
    def main(args: Array[String]) {
      if (args.length < 2) {
        System.err.println("Usage: NetworkWordCount <hostname> <port>")
        System.exit(1)
      }

      val sparkConf = new SparkConf().setAppName("NetworkWordCount").set("spark.cassandra.connection.host", "10.75.123.103")
     //      val sparkConf = new SparkConf().setAppName("NetworkWordCount").set("spark.cassandra.connection.host", "localhost").setMaster("local[2]")
      // val sc = new SparkContext(sparkConf)
      //sc.addJar("/Users/sharma.varun/flipkart/gdp/spark-1.1.0/spark-cassandra-connector/spark-cassandra-connector-java/target/scala-2.10/spark-cassandra-connector-java-assembly-1.2.0-SNAPSHOT.jar")
      //val collection = sc.parallelize(Seq(("cat", 30), ("fox", 40)))
      //collection.saveToCassandra("streaming_test", "words", SomeColumns("word", "count"))

      //StreamingExamples.setStreamingLogLevavenels()

      // Create the context with a 1 second batch size

      val ssc = new StreamingContext(sparkConf, Seconds(1))
      ssc.sparkContext.addJar("/opt/flipkart/spark-cassandra-connector/spark-cassandra-connector-java/target/scala-2.10/spark-cassandra-connector-java-assembly-1.2.0-SNAPSHOT.jar")
      //ssc.sparkContext.addJar("/Users/sharma.varun/flipkart/gdp/spark-1.1.0/spark-cassandra-connector/spark-cassandra-connector-java/target/scala-2.10/spark-cassandra-connector-java-assembly-1.2.0-SNAPSHOT.jar")
      // Create a socket stream on target ip:port and count the
      // words in input stream of \n delimited text (eg. generated by 'nc')
      // Note that no duplication in storage level only for running locally.
      // Replication necessary in distributed scenario for fault tolerance.
      val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
      val words = lines.flatMap(_.split(" "))
      words.print()
      val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
      wordCounts.print()
      wordCounts.saveToCassandra("demo", "wordcount", SomeColumns("word", "count"))
      ssc.start()
      ssc.awaitTermination()
    }
    }